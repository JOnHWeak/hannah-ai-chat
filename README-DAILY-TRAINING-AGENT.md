# ü§ñ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Agent Train AI Daily

## üìã T·ªïng Quan

H·ªá th·ªëng Daily Training Agent cho ph√©p AI t·ª± ƒë·ªông h·ªçc t·ª´ d·ªØ li·ªáu m·ªõi m·ªói ng√†y, bao g·ªìm:
- **Chat History** t·ª´ ng∆∞·ªùi d√πng (rating ‚â• 4)
- **Knowledge Base** t·ª´ Elasticsearch
- **Dataset** t·ª´ PowerPoint files
- **LoRA Fine-tuning** v·ªõi Unsloth
- **Model Conversion** sang GGUF format

## üéØ Workflow Daily Training

```mermaid
graph TD
    A[Daily Trigger] --> B[Export Chat Data]
    B --> C[Export KB Data]
    C --> D[Merge Datasets]
    D --> E[Train LoRA]
    E --> F[Merge & Convert]
    F --> G[Deploy to LM Studio]
    
    H[User Ratings] --> I[Chat History]
    I --> B
    
    J[Knowledge Base] --> K[Elasticsearch]
    K --> C
    
    L[Dataset Files] --> M[PowerPoint Import]
    M --> N[Knowledge Base]
    N --> K
```

## üöÄ C√°ch S·ª≠ D·ª•ng Agent

### Ph∆∞∆°ng Ph√°p 1: PowerShell Agent (Khuy·∫øn ngh·ªã)

```powershell
# Ch·∫°y agent t·ª± ƒë·ªông v·ªõi monitoring
.\scripts\run-agent.ps1 -Full

# Ch·ªâ ch·∫°y monitoring
.\scripts\run-agent.ps1 -Monitor

# Ch·ªâ ch·∫°y training
.\scripts\run-agent.ps1 -Training

# Ho·∫∑c s·ª≠ d·ª•ng script c≈©
.\scripts\daily-train.ps1
```

**Agent s·∫Ω t·ª± ƒë·ªông:**
1. ‚úÖ Export chat data t·ª´ database (rating ‚â• 4)
2. ‚úÖ Export knowledge base t·ª´ Elasticsearch
3. ‚úÖ Merge datasets th√†nh training data
4. ‚úÖ Train LoRA model v·ªõi Unsloth
5. ‚úÖ Convert sang GGUF format
6. ‚úÖ Deploy model m·ªõi v√†o LM Studio

### Ph∆∞∆°ng Ph√°p 2: Python Agent

```python
# T·∫°o agent training script
import subprocess
import os
from datetime import datetime

def run_daily_training():
    """Agent function ƒë·ªÉ ch·∫°y daily training"""
    
    print(f"ü§ñ Daily Training Agent started at {datetime.now()}")
    
    # Step 1: Export daily chat data
    print("üìä Step 1: Exporting daily chat data...")
    subprocess.run(["python", "scripts/data/export_daily_dataset.py"])
    
    # Step 2: Export KB data (optional)
    print("üìö Step 2: Exporting knowledge base data...")
    try:
        subprocess.run(["python", "scripts/sft/es_to_sft.py"])
    except Exception as e:
        print(f"‚ö†Ô∏è  KB export failed: {e}")
    
    # Step 3: Merge datasets
    print("üîÑ Step 3: Merging datasets...")
    # Logic merge datasets
    
    # Step 4: Train LoRA
    print("üéØ Step 4: Training LoRA model...")
    subprocess.run(["python", "scripts/train/train_lora_unsloth.py"])
    
    # Step 5: Convert to GGUF (optional)
    if os.getenv("LLAMA_CPP_DIR"):
        print("üîÑ Step 5: Converting to GGUF...")
        subprocess.run(["python", "scripts/train/merge_and_convert.py"])
    
    print("‚úÖ Daily training completed!")

if __name__ == "__main__":
    run_daily_training()
```

### Ph∆∞∆°ng Ph√°p 3: Task Scheduler Agent (Windows)

1. **M·ªü Task Scheduler:**
   - Windows + R ‚Üí `taskschd.msc`

2. **T·∫°o Task m·ªõi:**
   - Action ‚Üí Create Task
   - Name: "AI Daily Training Agent"

3. **C·∫•u h√¨nh Trigger:**
   - Triggers ‚Üí New
   - Daily at 2:00 AM (ho·∫∑c th·ªùi gian b·∫°n mu·ªën)

4. **C·∫•u h√¨nh Action:**
   - Actions ‚Üí New
   - Program: `PowerShell.exe`
   - Arguments: `-ExecutionPolicy Bypass -File "D:\hannah-ai-chat\hannah-ai-chat\scripts\daily-train.ps1"`

5. **C·∫•u h√¨nh Settings:**
   - Allow task to run on demand: ‚úÖ
   - Run task as soon as possible after a scheduled start is missed: ‚úÖ
   - If the task fails, restart every: 1 hour

## üìä C√°c Lo·∫°i D·ªØ Li·ªáu Training

### 1. Chat History Data
```json
{
  "messages": [
    {"role": "user", "content": "What is data structure?"},
    {"role": "assistant", "content": "A data structure is..."}
  ],
  "weight": 0.8
}
```

**Ngu·ªìn:** Chat interactions v·ªõi rating ‚â• 4  
**File:** `data/daily/YYYY-MM-DD.jsonl`

### 2. Knowledge Base Data
```json
{
  "messages": [
    {"role": "user", "content": "Explain: Data Structures"},
    {"role": "assistant", "content": "Data structures are..."}
  ],
  "weight": 0.6
}
```

**Ngu·ªìn:** Elasticsearch knowledge base  
**File:** `data/kb_es_sft.jsonl`

### 3. Dataset Data
```json
{
  "messages": [
    {"role": "user", "content": "Explain: Sorting Algorithms"},
    {"role": "assistant", "content": "Sorting algorithms are..."}
  ],
  "weight": 0.7
}
```

**Ngu·ªìn:** PowerPoint files t·ª´ dataset  
**File:** `data/kb_sft.jsonl`

## ‚öôÔ∏è C·∫•u H√¨nh Agent

### Environment Variables

```bash
# Training Configuration
BASE_MODEL=microsoft/Phi-4-mini-instruct
MAX_SEQ_LENGTH=4096
OUTPUT_DIR=artifacts/lora
DATA_FILE=data/daily/latest.jsonl

# LoRA Configuration
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05
TARGET_MODULES=all-linear

# Training Parameters
PER_DEVICE_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
NUM_TRAIN_EPOCHS=1
LEARNING_RATE=1e-4

# Model Conversion
LLAMA_CPP_DIR=C:/tools/llama.cpp
QUANTIZATION=Q4_K_M
LM_STUDIO_MODELS_DIR=%LOCALAPPDATA%/LM Studio/models
```

### Agent Configuration File

```yaml
# agent_config.yaml
training:
  base_model: "microsoft/Phi-4-mini-instruct"
  max_seq_length: 4096
  output_dir: "artifacts/lora"
  
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: "all-linear"
  
training_args:
  per_device_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  learning_rate: 1e-4
  logging_steps: 10
  save_strategy: "epoch"
  bf16: true
  
data_sources:
  chat_history:
    enabled: true
    min_rating: 4
    days_back: 1
  
  knowledge_base:
    enabled: true
    es_index: "kb_software_engineering"
    max_size: 500
  
  dataset:
    enabled: true
    dataset_dir: "D:/Data_set"
    categories: ["all"]

conversion:
  llama_cpp_dir: "C:/tools/llama.cpp"
  quantization: "Q4_K_M"
  lm_studio_deploy: true
```

## üîß Scripts Agent S·ª≠ D·ª•ng

### 1. Export Daily Dataset
```bash
python scripts/data/export_daily_dataset.py
```
**Ch·ª©c nƒÉng:** Export chat history c√≥ rating ‚â• 4 t·ª´ 24h qua

### 2. Export Knowledge Base
```bash
python scripts/sft/es_to_sft.py
```
**Ch·ª©c nƒÉng:** Export knowledge base t·ª´ Elasticsearch th√†nh SFT format

### 3. Train LoRA Model
```bash
python scripts/train/train_lora_unsloth.py
```
**Ch·ª©c nƒÉng:** Fine-tune model v·ªõi LoRA using Unsloth

### 4. Merge & Convert
```bash
python scripts/train/merge_and_convert.py
```
**Ch·ª©c nƒÉng:** Merge LoRA v√†o base model v√† convert sang GGUF

## üìà Monitoring Agent

### Log Files
```bash
# Training logs
artifacts/lora/training_logs.txt

# Agent execution logs
logs/daily_training_YYYY-MM-DD.log

# Error logs
logs/agent_errors.log
```

### Monitoring Script
```python
# monitor_agent.py
import os
import json
from datetime import datetime

def check_agent_status():
    """Ki·ªÉm tra tr·∫°ng th√°i agent"""
    
    # Check latest training data
    daily_dir = "data/daily"
    if os.path.exists(daily_dir):
        files = os.listdir(daily_dir)
        latest_file = max(files) if files else None
        print(f"üìä Latest training data: {latest_file}")
    
    # Check LoRA artifacts
    lora_dir = "artifacts/lora"
    if os.path.exists(lora_dir):
        print(f"‚úÖ LoRA model exists: {lora_dir}")
    else:
        print(f"‚ùå LoRA model missing: {lora_dir}")
    
    # Check GGUF model
    gguf_file = "artifacts/gguf/model.Q4_K_M.gguf"
    if os.path.exists(gguf_file):
        size_mb = os.path.getsize(gguf_file) / (1024*1024)
        print(f"‚úÖ GGUF model exists: {size_mb:.1f} MB")
    else:
        print(f"‚ùå GGUF model missing: {gguf_file}")
    
    # Check LM Studio deployment
    lm_studio_path = f"{os.environ.get('LOCALAPPDATA')}/LM Studio/models/phi-4-mini-reasoning-daily"
    if os.path.exists(lm_studio_path):
        print(f"‚úÖ LM Studio deployment: {lm_studio_path}")
    else:
        print(f"‚ùå LM Studio deployment missing: {lm_studio_path}")

if __name__ == "__main__":
    check_agent_status()
```

## üö® Troubleshooting Agent

### L·ªói Th∆∞·ªùng G·∫∑p

1. **"No training data found"**
   ```bash
   # Ki·ªÉm tra chat history c√≥ rating
   python -c "from database import SessionLocal; from models import ChatHistory; db = SessionLocal(); print(db.query(ChatHistory).filter(ChatHistory.rating >= 4).count())"
   ```

2. **"CUDA out of memory"**
   ```bash
   # Gi·∫£m batch size
   export PER_DEVICE_BATCH_SIZE=1
   export GRADIENT_ACCUMULATION_STEPS=16
   ```

3. **"Elasticsearch connection failed"**
   ```bash
   # Ki·ªÉm tra ES status
   curl http://localhost:9200/_cluster/health
   ```

4. **"LoRA training failed"**
   ```bash
   # Ki·ªÉm tra dependencies
   pip install unsloth transformers datasets trl
   ```

### Agent Health Check

```bash
# Ch·∫°y health check
python -c "
import sys
sys.path.append('.')
from scripts.data.export_daily_dataset import main as export_daily
from scripts.sft.es_to_sft import main as export_kb
from scripts.train.train_lora_unsloth import main as train_lora

print('üîç Agent Health Check')
print('=' * 30)

try:
    export_daily()
    print('‚úÖ Daily export: OK')
except Exception as e:
    print(f'‚ùå Daily export: {e}')

try:
    export_kb()
    print('‚úÖ KB export: OK')
except Exception as e:
    print(f'‚ùå KB export: {e}')

try:
    train_lora()
    print('‚úÖ LoRA training: OK')
except Exception as e:
    print(f'‚ùå LoRA training: {e}')
"
```

## üéØ Best Practices

### 1. Data Quality
- ‚úÖ Ch·ªâ train v·ªõi chat c√≥ rating ‚â• 4
- ‚úÖ Filter out spam v√† low-quality content
- ‚úÖ Balance gi·ªØa c√°c lo·∫°i d·ªØ li·ªáu

### 2. Training Schedule
- ‚úÖ Ch·∫°y daily v√†o 2-3 AM (√≠t traffic)
- ‚úÖ Monitor GPU memory usage
- ‚úÖ Backup model tr∆∞·ªõc khi train

### 3. Model Management
- ‚úÖ Version control cho models
- ‚úÖ A/B testing v·ªõi models kh√°c nhau
- ‚úÖ Rollback mechanism n·∫øu model m·ªõi t·ªá h∆°n

### 4. Monitoring
- ‚úÖ Track training metrics
- ‚úÖ Monitor model performance
- ‚úÖ Alert khi c√≥ l·ªói

## üìä Performance Metrics

### Training Metrics
```python
# training_metrics.py
def track_training_metrics():
    metrics = {
        "training_samples": count_training_samples(),
        "training_time": measure_training_time(),
        "gpu_memory_usage": get_gpu_memory(),
        "model_size": get_model_size(),
        "convergence_loss": get_final_loss()
    }
    return metrics
```

### Model Performance
```python
# model_performance.py
def evaluate_model_performance():
    # Test tr√™n validation set
    # So s√°nh v·ªõi model c≈©
    # Track user ratings
    pass
```

## üîÆ Advanced Features

### 1. Multi-GPU Training
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3
export WORLD_SIZE=4
```

### 2. Distributed Training
```bash
python -m torch.distributed.launch --nproc_per_node=4 scripts/train/train_lora_unsloth.py
```

### 3. Hyperparameter Tuning
```python
# Tune learning rate, batch size, LoRA parameters
# S·ª≠ d·ª•ng Optuna ho·∫∑c Ray Tune
```

### 4. Model Ensembling
```python
# Combine multiple LoRA models
# Weighted voting ho·∫∑c averaging
```

---

üéâ **Agent Daily Training ho√†n ch·ªânh!** AI c·ªßa b·∫°n s·∫Ω t·ª± ƒë·ªông h·ªçc v√† c·∫£i thi·ªán m·ªói ng√†y t·ª´ d·ªØ li·ªáu m·ªõi!
